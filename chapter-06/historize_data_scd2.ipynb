{"cells":[{"cell_type":"code","source":["# Import functions\n","from pyspark import *\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import *\n","import datetime"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"9209489c-df0b-4f9c-92c5-14075561863b","normalized_state":"finished","queued_time":"2025-01-21T14:49:23.068783Z","session_start_time":"2025-01-21T14:49:23.7485661Z","execution_start_time":"2025-01-21T14:49:34.5382979Z","execution_finish_time":"2025-01-21T14:49:37.3935749Z","parent_msg_id":"4d3ba149-21dc-48b5-9eb6-f0696a8e0346"},"text/plain":"StatementMeta(, 9209489c-df0b-4f9c-92c5-14075561863b, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0b100ce-54e0-45b0-9c46-e76abcf41c4f"},{"cell_type":"code","source":["# SCD2 function\n","def fn_SCD2(schemaName, tableName, primaryKey):\n","\n","    # Fetch data from Bronze or intermediate Silver layer\n","    dataChanged = spark.read.table(f\"{schemaName}.clean_{tableName}\") \n","\n","    # Remove loading_date column from dataset\n","    dataChanged = dataChanged.drop('loading_date')\n","\n","    # Generate hash key if primary is missing\n","    if not primaryKey or primaryKey == \"\":\n","        dataChanged = dataChanged.withColumn(\"hash\", \\\n","        sha2(concat_ws(\"||\", *dataChanged.columns), 256))\n","        primaryKey = 'hash'\n","\n","    # Create list with all columns\n","    columnNames = dataChanged.schema.names\n","\n","    # Set date\n","    current_date = datetime.date.today()\n","\n","    # Try and read existing dataset\n","    try:\n","        # Read original data - this is your scd type 2 table holding all data\n","        dataOriginal = spark.read.table(f\"{schemaName}.hist_{tableName}\")\n","    except:\n","        # Use first load when no data exists yet\n","        newOriginalData = dataChanged.withColumn('current', lit(True)) \\\n","        .withColumn('effectiveDate', lit(current_date)) \\\n","        .withColumn('endDate', lit(datetime.date(9999, 12, 31)))\n","        newOriginalData.write.format(\"delta\").mode(\"overwrite\") \\\n","        .saveAsTable(f\"{schemaName}.hist_{tableName}\")\n","\n","    # Read original data - this is your scd type 2 table holding all data\n","    dataOriginal = spark.read.table(f\"{schemaName}.hist_{tableName}\")\n","\n","    # Rename all columns in dataChanged, prepend src_ to column names\n","    df_new = dataChanged.select([F.col(c).alias(\"src_\"+c) \\\n","    for c in dataChanged.columns])\n","    src_columnNames = df_new.schema.names\n","    df_new2 = df_new.withColumn('src_current', lit(True)) \\\n","    .withColumn('src_effectiveDate', lit(current_date)) \\\n","    .withColumn('src_endDate', lit(datetime.date(9999, 12, 31)))\n","\n","    # Create dynamic columns\n","    src_primaryKey = 'src_' + primaryKey\n","\n","    # FULL Merge, join on key column and also \n","    # date column to make only join to the latest records\n","    df_merge = dataOriginal.join(df_new2, (df_new2[src_primaryKey] \\\n","    == dataOriginal[primaryKey]), how='fullouter')\n","\n","    # Derive new column to indicate the action\n","    df_merge = df_merge.withColumn('action',\n","        when(concat_ws('+', *columnNames) == \\\n","        concat_ws('+', *src_columnNames), 'NOACTION')\n","        .when(df_merge.current == False, 'NOACTION')\n","        .when(df_merge[src_primaryKey].isNull() & df_merge.current, 'DELETE')\n","        .when(df_merge[src_primaryKey].isNull(), 'INSERT')\n","        .otherwise('UPDATE')\n","    )\n","\n","    # Generate target selections based on action codes\n","    column_names = columnNames + ['current', 'effectiveDate', 'endDate']\n","    src_column_names = src_columnNames + ['src_current', \\\n","    'src_effectiveDate', 'src_endDate']\n","\n","    # For records that needs no action\n","    df_merge_p1 = df_merge.filter(df_merge.action == \\\n","    'NOACTION').select(column_names)\n","\n","    # For records that needs insert only\n","    df_merge_p2 = df_merge.filter(df_merge.action == \\\n","    'INSERT').select(src_column_names)\n","    df_merge_p2_1 = df_merge_p2.select([F.col(c) \\\n","    .alias(c.replace(c[0:4], \"\")) for c in df_merge_p2.columns])\n","\n","    # For records that needs to be deleted\n","    df_merge_p3 = df_merge.filter(df_merge.action == \\\n","    'DELETE').select(column_names).withColumn('current', lit(False)) \\\n","    .withColumn('endDate', lit(current_date))\n","\n","    # For records that needs to be expired and then inserted\n","    df_merge_p4_1 = df_merge.filter(df_merge.action == \\\n","    'UPDATE').select(src_column_names)\n","    df_merge_p4_2 = df_merge_p4_1.select([F.col(c) \\\n","    .alias(c.replace(c[0:4], \"\")) for c in df_merge_p2.columns])\n","\n","    # Replace src_ alias in all columns\n","    df_merge_p4_3 = df_merge.filter(df_merge.action == \\\n","    'UPDATE').withColumn('endDate', date_sub(df_merge.src_effectiveDate, 1)) \\\n","    .withColumn('current', lit(False)).select(column_names)\n","\n","    # Union all records together\n","    df_merge_final = df_merge_p1.unionAll(df_merge_p2) \\\n","    .unionAll(df_merge_p3).unionAll(df_merge_p4_2).unionAll(df_merge_p4_3)\n","\n","    # At last, you can overwrite existing data using this new data frame\n","    df_merge_final.write.format(\"delta\").mode(\"overwrite\") \\\n","    .saveAsTable(schemaName + \".hist_\" + tableName)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"9209489c-df0b-4f9c-92c5-14075561863b","normalized_state":"finished","queued_time":"2025-01-21T14:54:06.5657039Z","session_start_time":null,"execution_start_time":"2025-01-21T14:54:06.9467102Z","execution_finish_time":"2025-01-21T14:54:07.2227823Z","parent_msg_id":"ae64878e-9714-40f5-ac1c-161c4d0965a2"},"text/plain":"StatementMeta(, 9209489c-df0b-4f9c-92c5-14075561863b, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"addd8fe3-00cf-44f9-9b73-58ffff46a150"},{"cell_type":"code","source":["fn_SCD2(\"adventureworks\",\"address\",\"AddressID\")\n","fn_SCD2(\"adventureworks\",\"customer\",\"CustomerID\")\n","fn_SCD2(\"adventureworks\",\"customeraddress\",\"\")\n","fn_SCD2(\"adventureworks\",\"product\",\"ProductID\")\n","fn_SCD2(\"adventureworks\",\"productcategory\",\"ProductCategoryID\")\n","fn_SCD2(\"adventureworks\",\"productdescription\",\"ProductDescriptionID\")\n","fn_SCD2(\"adventureworks\",\"productmodel\",\"ProductModelID\")\n","fn_SCD2(\"adventureworks\",\"productmodelproductdescription\",\"\")\n","fn_SCD2(\"adventureworks\",\"salesorderdetail\",\"\")\n","fn_SCD2(\"adventureworks\",\"salesorderheader\",\"SalesOrderID\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"9209489c-df0b-4f9c-92c5-14075561863b","normalized_state":"finished","queued_time":"2025-01-21T14:54:32.7797778Z","session_start_time":null,"execution_start_time":"2025-01-21T14:54:33.0908531Z","execution_finish_time":"2025-01-21T14:55:59.3410839Z","parent_msg_id":"682d3dfe-f105-41aa-b9d4-4ab6a445e7b8"},"text/plain":"StatementMeta(, 9209489c-df0b-4f9c-92c5-14075561863b, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1d356f1-dcda-4a6c-a009-f3cc8f2cd65b"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"notebook_environment":{},"widgets":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5e948e85-b798-4967-bdad-a26c0a44d0cd"}],"default_lakehouse":"5e948e85-b798-4967-bdad-a26c0a44d0cd","default_lakehouse_name":"Silver","default_lakehouse_workspace_id":"30950d63-22f3-4d65-8813-310477df47b4"}}},"nbformat":4,"nbformat_minor":5}