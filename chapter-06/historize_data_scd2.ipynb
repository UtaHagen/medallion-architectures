{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b100ce-54e0-45b0-9c46-e76abcf41c4f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-24T10:29:44.5879838Z",
       "execution_start_time": "2024-11-24T10:29:41.8410777Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "84ff9b0a-d144-40a9-9e06-3263f3a87d36",
       "queued_time": "2024-11-24T10:29:27.9910641Z",
       "session_id": "6c8d5796-d4aa-489d-be93-a5980931c4a0",
       "session_start_time": "2024-11-24T10:29:28.1776939Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 6c8d5796-d4aa-489d-be93-a5980931c4a0, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import functions\n",
    "from pyspark import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f09d71-bf17-4420-b012-b2239db201af",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-24T10:29:46.4047531Z",
       "execution_start_time": "2024-11-24T10:29:46.1345813Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5227cfcc-f46f-4838-8982-365d0e937be3",
       "queued_time": "2024-11-24T10:29:45.8157645Z",
       "session_id": "6c8d5796-d4aa-489d-be93-a5980931c4a0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 6c8d5796-d4aa-489d-be93-a5980931c4a0, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fn_SCD2(schemaName, tableName, primaryKey):\n",
    "    # Fetch data from Bronze or intermediate Silver layer\n",
    "    dataChanged = spark.read.table(f\"{schemaName}.clean_{tableName}\") \n",
    "\n",
    "    # Remove loading_date column from dataset\n",
    "    dataChanged = dataChanged.drop('loading_date')\n",
    "\n",
    "    # Generate hash key if primary is missing\n",
    "    if not primaryKey or primaryKey == \"\":\n",
    "        dataChanged = dataChanged.withColumn(\"hash\", \\\n",
    "        sha2(concat_ws(\"||\", *dataChanged.columns), 256))\n",
    "        primaryKey = 'hash'\n",
    "\n",
    "    # Create list with all columns\n",
    "    columnNames = dataChanged.schema.names\n",
    "\n",
    "    # Set date\n",
    "    import datetime\n",
    "    current_date = datetime.date.today()\n",
    "\n",
    "    # Try and read existing dataset\n",
    "    try:\n",
    "        fn_merge(schemaName, tableName, primaryKey)\n",
    "    except:\n",
    "        # Use first load when no data exists yet\n",
    "        newOriginalData = dataChanged.withColumn('current', lit(True)) \\\n",
    "        .withColumn('effectiveDate', lit(current_date)) \\\n",
    "        .withColumn('endDate', lit(datetime.date(9999, 12, 31)))\n",
    "        newOriginalData.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"{schemaName}.hist_{tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3b7790-e497-4354-9dbb-5fdc2b4f7f9d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-24T10:29:48.9029535Z",
       "execution_start_time": "2024-11-24T10:29:48.6682324Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5ed7dd35-1ce7-43c8-b52c-cc3dd2c79c30",
       "queued_time": "2024-11-24T10:29:48.3511363Z",
       "session_id": "6c8d5796-d4aa-489d-be93-a5980931c4a0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 6c8d5796-d4aa-489d-be93-a5980931c4a0, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fn_merge(schemaName, tableName, primaryKey):\n",
    "    # Read original data - this is your scd type 2 table holding all data\n",
    "    dataOriginal = spark.sql(f\"SELECT * FROM {schemaName}.hist_{tableName}\")\n",
    "\n",
    "    # Rename all columns in dataChanged, prepend src_ to column names\n",
    "    df_new = dataChanged.select([F.col(c).alias(\"src_\"+c) \\\n",
    "    for c in dataChanged.columns])\n",
    "    src_columnNames = df_new.schema.names\n",
    "    df_new2 = df_new.withColumn('src_current', lit(True)) \\\n",
    "    .withColumn('src_effectiveDate', lit(current_date)) \\\n",
    "    .withColumn('src_endDate', lit(datetime.date(9999, 12, 31)))\n",
    "\n",
    "    # Create dynamic columns\n",
    "    src_primaryKey = 'src_' + primaryKey\n",
    "\n",
    "    # FULL Merge, join on key column and also \n",
    "    # date column to make only join to the latest records\n",
    "    df_merge = dataOriginal.join(df_new2, (df_new2[src_primaryKey] \\\n",
    "    == dataOriginal[primaryKey]), how='fullouter')\n",
    "\n",
    "    # Derive new column to indicate the action\n",
    "    df_merge = df_merge.withColumn('action',\n",
    "        when(concat_ws('+', *columnNames) == \\\n",
    "        concat_ws('+', *src_columnNames), 'NOACTION')\n",
    "        .when(df_merge.current == False, 'NOACTION')\n",
    "        .when(df_merge[src_primaryKey].isNull() & df_merge.current, 'DELETE')\n",
    "        .when(df_merge[src_primaryKey].isNull(), 'INSERT')\n",
    "        .otherwise('UPDATE')\n",
    "    )\n",
    "\n",
    "    # Generate target selections based on action codes\n",
    "    column_names = columnNames + ['current', 'effectiveDate', 'endDate']\n",
    "    src_column_names = src_columnNames + ['src_current', \\\n",
    "    'src_effectiveDate', 'src_endDate']\n",
    "\n",
    "    # For records that needs no action\n",
    "    df_merge_p1 = df_merge.filter(df_merge.action == \\\n",
    "    'NOACTION').select(column_names)\n",
    "\n",
    "    # For records that needs insert only\n",
    "    df_merge_p2 = df_merge.filter(df_merge.action == \\\n",
    "    'INSERT').select(src_column_names)\n",
    "    df_merge_p2_1 = df_merge_p2.select([F.col(c) \\\n",
    "    .alias(c.replace(c[0:4], \"\")) for c in df_merge_p2.columns])\n",
    "\n",
    "    # For records that needs to be deleted\n",
    "    df_merge_p3 = df_merge.filter(df_merge.action == \\\n",
    "    'DELETE').select(column_names).withColumn('current', lit(False)) \\\n",
    "    .withColumn('endDate', lit(current_date))\n",
    "\n",
    "    # For records that needs to be expired and then inserted\n",
    "    df_merge_p4_1 = df_merge.filter(df_merge.action == \\\n",
    "    'UPDATE').select(src_column_names)\n",
    "    df_merge_p4_2 = df_merge_p4_1.select([F.col(c) \\\n",
    "    .alias(c.replace(c[0:4], \"\")) for c in df_merge_p2.columns])\n",
    "\n",
    "    # Replace src_ alias in all columns\n",
    "    df_merge_p4_3 = df_merge.filter(df_merge.action == \\\n",
    "    'UPDATE').withColumn('endDate', date_sub(df_merge.src_effectiveDate, 1)) \\\n",
    "    .withColumn('current', lit(False)).select(column_names)\n",
    "\n",
    "    # Union all records together\n",
    "    df_merge_final = df_merge_p1.unionAll(df_merge_p2) \\\n",
    "    .unionAll(df_merge_p3).unionAll(df_merge_p4_2).unionAll(df_merge_p4_3)\n",
    "\n",
    "    # At last, you can overwrite existing data using this new data frame\n",
    "    df_merge_final.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(schemaName + \".hist_\" + tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09de7da-d3ae-4d19-90ed-adf652b94840",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-24T10:30:56.145051Z",
       "execution_start_time": "2024-11-24T10:29:51.7284846Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2ac53214-5316-4b88-8a58-549501df6682",
       "queued_time": "2024-11-24T10:29:51.4209021Z",
       "session_id": "6c8d5796-d4aa-489d-be93-a5980931c4a0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 6c8d5796-d4aa-489d-be93-a5980931c4a0, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn_SCD2(\"adventureworks\",\"address\",\"AddressID\")\n",
    "fn_SCD2(\"adventureworks\",\"customer\",\"CustomerID\")\n",
    "fn_SCD2(\"adventureworks\",\"customeraddress\",\"\")\n",
    "fn_SCD2(\"adventureworks\",\"product\",\"ProductID\")\n",
    "fn_SCD2(\"adventureworks\",\"productcategory\",\"ProductCategoryID\")\n",
    "fn_SCD2(\"adventureworks\",\"productdescription\",\"ProductDescriptionID\")\n",
    "fn_SCD2(\"adventureworks\",\"productmodel\",\"ProductModelID\")\n",
    "fn_SCD2(\"adventureworks\",\"productmodelproductdescription\",\"\")\n",
    "fn_SCD2(\"adventureworks\",\"salesorderdetail\",\"\")\n",
    "fn_SCD2(\"adventureworks\",\"salesorderheader\",\"SalesOrderID\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "5e948e85-b798-4967-bdad-a26c0a44d0cd",
    "default_lakehouse_name": "Silver",
    "default_lakehouse_workspace_id": "30950d63-22f3-4d65-8813-310477df47b4",
    "known_lakehouses": [
     {
      "id": "5e948e85-b798-4967-bdad-a26c0a44d0cd"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    },
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
