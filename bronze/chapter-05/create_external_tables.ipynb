{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch parameters from Data Factory\n",
    "schemaName=dbutils.widgets.get(\"schemaName\")\n",
    "tableName=dbutils.widgets.get(\"tableName\")\n",
    "filePath=dbutils.widgets.get(\"filePath\")\n",
    "\n",
    "# Create database\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS bronze_{schemaName}')\n",
    "\n",
    "# Drop table\n",
    "spark.sql(f'DROP TABLE IF EXISTS bronze_{schemaName}.{tableName}')\n",
    "\n",
    "# Create new external table using latest datetime location\n",
    "ddl_query = f\"\"\"\n",
    "  CREATE TABLE bronze_{schemaName}.{tableName} \n",
    "  USING PARQUET\n",
    "  LOCATION '/mnt/bronze/\n",
    "  {schemaName}/{filePath}/{tableName}.parquet'\n",
    "\"\"\"\n",
    "\n",
    "# Execute query\n",
    "spark.sql(ddl_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using external tables is simple and straightforward; it does not require data duplication. However, you must keep several considerations in mind:\n",
    "\n",
    "\n",
    "- Reading from external Parquet tables is generally slower than reading from Delta tables. This performance lag can be attributed to factors like suboptimal partitioning, which affects how data is accessed and processed.\n",
    "\n",
    "\n",
    "- Unlike managed Delta tables, external Parquet tables lack support for advanced features such as schema enforcement and time travel, which can enhance data management and analytics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
